import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, OrdinalEncoder
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

# Step 1: Load dataset
data = pd.read_csv("AttritionTest.csv")

# Step 2: Encode variables
# Label encode binary 'Left' column
data['Left'] = data['Left'].map({'Yes': 1, 'No': 0})

# Ordinal encode 'LM Rating' and 'Rating' columns
ordinal_columns = ['LM Rating', 'Rating']
ordinal_categories = [['Blank', 'Not Achieved', 'Achieved', 'Over Achieved', 'Exceptional'],
                      ['Blank', 'Not Achieved', 'Achieved', 'Over Achieved', 'Exceptional']]
ordinal_encoder = OrdinalEncoder(categories=ordinal_categories)
data[ordinal_columns] = ordinal_encoder.fit_transform(data[ordinal_columns])

# One-hot encode the nominal columns
nominal_columns = ['Directorate', 'Location', 'JobFamilyType', 'Gender', 'Ethnicity', 'JobFamily']
data_encoded = pd.get_dummies(data, columns=nominal_columns)

# Step 3: Prepare features and target
X = data_encoded.drop(['Dummy E number', 'Left'], axis=1)
y = data_encoded['Left']

# Step 4: Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 5: Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Step 6: Prepare data for LSTM (time-series analysis)
time_steps = 12  # Number of months to consider as input
n_features = X_train_scaled.shape[1]

# Generate sequences for LSTM input
def generate_sequences(X, y, time_steps):
    X_seq, y_seq = [], []
    for i in range(len(X) - time_steps):
        X_seq.append(X[i:i + time_steps])
        y_seq.append(y[i + time_steps])
    return np.array(X_seq), np.array(y_seq)

X_train_seq, y_train_seq = generate_sequences(X_train_scaled, y_train, time_steps)
X_test_seq, y_test_seq = generate_sequences(X_test_scaled, y_test, time_steps)

# Step 7: Build LSTM model
model = Sequential()
model.add(LSTM(64, input_shape=(time_steps, n_features)))
model.add(Dropout(0.2))
model.add(Dense(1, activation='sigmoid'))

# Step 8: Compile and train LSTM model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
early_stopping = EarlyStopping(patience=3, monitor='val_loss', restore_best_weights=True)
history = model.fit(X_train_seq, y_train_seq, epochs=20, batch_size=16, validation_data=(X_test_seq, y_test_seq),
                    callbacks=[early_stopping])

# Step 9: Evaluate LSTM model
loss, accuracy = model.evaluate(X_test_seq, y_test_seq)
print(f"Accuracy: {accuracy}")

# Step 10: Make predictions on full dataset
full_data = pd.read_csv("Attrition_Test_FULL.csv")
full_data['Left'] = full_data['Left'].map({'Yes': 1, 'No': 0})
full_data[ordinal_columns] = ordinal_encoder.transform(full_data[ordinal_columns])
full_data_encoded = pd.get_dummies(full_data, columns=nominal_columns)
X_full = full_data_encoded.drop(['Dummy E number', 'Left'], axis=1)
X_full_scaled = scaler.transform(X_full)
X_full_seq, _ = generate_sequences(X_full_scaled, np.zeros(len(X_full_scaled)), time_steps)  # Dummy y values
full_predictions = model.predict(X_full_seq)

# Step 11: Create DataFrame with results
full_results_df = pd.DataFrame({
    'Dummy E number': full_data['Dummy E number'],
    'Probability_of_Leaving': full_predictions.flatten(),
    'Left': full_data['Left']
})
print(full_results_df)

# Step 12: Plot training and validation loss
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
