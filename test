import pandas as pd
from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, OrdinalEncoder
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score
from sklearn.utils import class_weight

def evaluate_model(model, X_test, y_test): 
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred) 
    print(f"Accuracy: {accuracy}") 
    print(confusion_matrix(y_test,y_pred))
    print("\nClassification Report: ") 
    print(classification_report(y_test, y_pred))
    return accuracy, y_pred

# Step 1: Load dataset
data = pd.read_csv("AttritionTest.csv")

# Step 2: Encode variables
# Label encode binary 'Left' column
data['Left'] = data['Left'].map({'Yes': 1, 'No': 0})

# Ordinal encode 'LM Rating' and 'Rating' columns
ordinal_columns = ['LM Rating', 'Rating']
ordinal_categories = [['Blank', 'Not Achieved', 'Achieved', 'Over Achieved', 'Exceptional'], 
                      ['Blank', 'Not Achieved', 'Achieved', 'Over Achieved', 'Exceptional']]
ordinal_encoder = OrdinalEncoder(categories=ordinal_categories)
data[ordinal_columns] = ordinal_encoder.fit_transform(data[ordinal_columns])

# One-hot encode the nominal columns
nominal_columns = ['Directorate', 'Location', 'JobFamilyType', 'Gender', 'Ethnicity', 'JobFamily']
data_encoded = pd.get_dummies(data, columns=nominal_columns)

# Step 3: Prepare features and target
X = data_encoded.drop(['EmployeeID', 'Left'], axis = 1)
y = data_encoded['Left']

# Step 4: Split data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 5: Feature scaling
scaler = StandardScaler() 
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Step 6: Class weights
sample_weights = class_weight.compute_sample_weight ('balanced', y_train)

# Step 7: Fine tune parameters
# Use RandomizedSearchCV for hyperparameter tuning
param_dist = {
    'n_estimators': [50,100,200],
    'learning_rate': [0.01,0.1,1],
    'min_samples_split': [2,5,10],
    'min_samples_leaf': [1,2,4],
    'max_depth': [3,5,10]
}

# Define cross-validation strategy
cv = KFold(n_splits=5, shuffle=True, random_state=42)

# Step 8: Create Classifier
clf = GradientBoostingClassifier(random_state=21)

# Step 9: Train and evaluate classifiers 
random_search = RandomizedSearchCV(clf, param_distributions=param_dist, n_iter=15, cv=cv)
random_search.fit(X_train_scaled, y_train, sample_weight=sample_weights)
evaluate_model(random_search, X_test_scaled, y_test)

# Compute predicted probabilities
y_pred_proba = random_search.predict_proba(X_test_scaled)[:, 1]

# Compute AUC-ROC
auc_roc =roc_auc_score(y_test, y_pred_proba)
print(f"AUC-ROC: {auc_roc}")

import matplotlib.pyplot as plt
import numpy as np

# Get the probabilities of class 1
prob_class_1 = y_pred_proba

# Generate the histogram
plt.hist(prob_class_1, bins=np.linspace(0, 1, 21), alpha=0.5, color='b', edgecolor='black')
plt.title('Histogram of predicted probabilities')
plt.xlabel('Predicted probability of Class 1')
plt.ylabel('Frequency')
plt.show()

# Next, let's see the distribution of probabilities for the actual leavers
leavers_probs = prob_class_1[y_test == 1]
plt.hist(leavers_probs, bins=np.linspace(0, 1, 21), alpha=0.5, color='r', edgecolor='black')
plt.title('Histogram of predicted probabilities for actual leavers')
plt.xlabel('Predicted probability of Class 1')
plt.ylabel('Frequency')
plt.show()

# Create a DataFrame with actual and predicted values
results_df = pd.DataFrame({
    'Actual': y_test,
    'Probability_of_1': y_pred_proba
})

# To get a better look at the data, you may want to sort the DataFrame by the probability:
results_df = results_df.sort_values(by='Probability_of_1', ascending=False)

print(results_df)
