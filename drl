import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import Adam
from collections import deque
import random

# Step 1: Load and preprocess the data
data = pd.read_csv("employee_data.csv")
state_features = ['age', 'gender', 'job_role', 'performance_rating']
state_data = data[state_features].fillna(0)  # Fill missing values with zeros or appropriate strategy
state_vectors = state_data.values

# Step 2: Define the action space
action_space = ['Retain', 'Intervene']
num_actions = len(action_space)

# Step 3: Define the Deep Q-Network (DQN) model
model = Sequential()
model.add(Dense(32, activation='relu', input_shape=(len(state_features),)))
model.add(Dense(32, activation='relu'))
model.add(Dense(num_actions, activation='linear'))
model.compile(loss='mse', optimizer=Adam(learning_rate=0.001))

# Step 4: Implement the replay memory
replay_memory = deque(maxlen=1000)

# Step 5: Define the training loop
num_episodes = 1000
epsilon = 1.0
epsilon_decay = 0.995
epsilon_min = 0.01
discount_factor = 0.99
batch_size = 32

for episode in range(num_episodes):
    state = state_vectors[np.random.randint(0, len(state_vectors))]  # Random initial state
    done = False
    total_reward = 0

    while not done:
        if np.random.rand() <= epsilon:
            action = np.random.choice(action_space)  # Random action for exploration
        else:
            q_values = model.predict(np.expand_dims(state, axis=0))
            action = action_space[np.argmax(q_values[0])]  # Action with the highest Q-value

        # Perform the action and calculate the reward
        if action == 'Retain':
            # Implement retain logic (e.g., no intervention)
            reward = 1  # Positive reward for retaining
        else:  # 'Intervene'
            # Implement intervention logic (e.g., simulated intervention cost)
            reward = -1  # Negative reward for intervention

        next_state = state_vectors[np.random.randint(0, len(state_vectors))]  # Random next state
        total_reward += reward

        # Store the transition in the replay memory
        replay_memory.append((state, action, reward, next_state, done))

        state = next_state

        # Train the model on a batch of experiences from replay memory
        if len(replay_memory) >= batch_size:
            batch = np.array(random.sample(replay_memory, batch_size))
            states, actions, rewards, next_states, dones = np.stack(batch[:, 0]), \
                                                           batch[:, 1], \
                                                           batch[:, 2], \
                                                           np.stack(batch[:, 3]), \
                                                           batch[:, 4]

            q_values = model.predict(states)
            next_q_values = model.predict(next_states)

            targets = q_values.copy()
            targets[np.arange(batch_size), np.where(actions == 'Intervene')] = rewards + \
                                                                               discount_factor * np.max(
                                                                                   next_q_values, axis=1)[
                                                                                   dones == False]

            # Perform a single gradient update on the batch
            model.fit(states, targets, epochs=1, verbose=0)

    # Decay epsilon
    epsilon = max(epsilon_min, epsilon * epsilon_decay)

    # Print episode results
    print(f"Episode {episode + 1}: Total Reward = {total_reward}")
